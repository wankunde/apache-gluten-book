# ç¬¬5ç« ï¼šæŸ¥è¯¢è®¡åˆ’è½¬æ¢

> **æœ¬ç« è¦ç‚¹**ï¼š
> - ç†è§£ Spark Physical Plan çš„ç»“æ„å’Œç‰¹ç‚¹
> - æŒæ¡ Substrait åè®®è§„èŒƒå’Œè®¾è®¡ç†å¿µ
> - æ·±å…¥å­¦ä¹  Spark Plan åˆ° Substrait Plan çš„è½¬æ¢è¿‡ç¨‹
> - äº†è§£ Substrait Plan åˆ° Native Plan çš„æ˜ å°„
> - æŒæ¡è®¡åˆ’è½¬æ¢çš„ä¼˜åŒ–ç­–ç•¥
> - ç†è§£ä¸æ”¯æŒç®—å­çš„è¯†åˆ«å’Œå¤„ç†

## å¼•è¨€

æŸ¥è¯¢è®¡åˆ’è½¬æ¢æ˜¯ Gluten æœ€æ ¸å¿ƒçš„åŠŸèƒ½ä¹‹ä¸€ã€‚å®ƒå°±åƒä¸€ä¸ª"ç¿»è¯‘å™¨"ï¼Œå°† Spark çš„æ‰§è¡Œè®¡åˆ’ç¿»è¯‘æˆåŸç”Ÿå¼•æ“èƒ½å¤Ÿç†è§£å’Œæ‰§è¡Œçš„æ ¼å¼ã€‚æœ¬ç« å°†æ·±å…¥å‰–æè¿™ä¸ªè½¬æ¢è¿‡ç¨‹çš„æ¯ä¸ªç»†èŠ‚ã€‚

## 5.1 Spark Physical Plan ä»‹ç»

### 5.1.1 Catalyst ä¼˜åŒ–å™¨å›é¡¾

Spark çš„ Catalyst ä¼˜åŒ–å™¨é‡‡ç”¨æ ‘å½¢ç»“æ„è¡¨ç¤ºæŸ¥è¯¢è®¡åˆ’ï¼š

```
é€»è¾‘è®¡åˆ’æ ‘
    â†“ (Analyzer)
å·²è§£æçš„é€»è¾‘è®¡åˆ’
    â†“ (Optimizer)
ä¼˜åŒ–åçš„é€»è¾‘è®¡åˆ’
    â†“ (Planner)
ç‰©ç†è®¡åˆ’æ ‘
```

### 5.1.2 Physical Plan ç»“æ„

Physical Plan æ˜¯ä¸€æ£µç”± SparkPlan èŠ‚ç‚¹ç»„æˆçš„æ ‘ï¼š

```scala
abstract class SparkPlan extends QueryPlan[SparkPlan] {
  // æ‰§è¡Œæ–¹æ³•
  def execute(): RDD[InternalRow]
  
  // åˆ—å¼æ‰§è¡Œï¼ˆå¦‚æœæ”¯æŒï¼‰
  def executeColumnar(): RDD[ColumnarBatch]
  
  // å­èŠ‚ç‚¹
  def children: Seq[SparkPlan]
}
```

**å¸¸è§ç®—å­ç¤ºä¾‹**ï¼š

```scala
// Filter ç®—å­
case class FilterExec(
  condition: Expression,
  child: SparkPlan
) extends UnaryExecNode

// Project ç®—å­
case class ProjectExec(
  projectList: Seq[NamedExpression],
  child: SparkPlan
) extends UnaryExecNode

// HashAggregate ç®—å­
case class HashAggregateExec(
  requiredChildDistributionExpressions: Option[Seq[Expression]],
  groupingExpressions: Seq[NamedExpression],
  aggregateExpressions: Seq[AggregateExpression],
  aggregateAttributes: Seq[Attribute],
  initialInputBufferOffset: Int,
  resultExpressions: Seq[NamedExpression],
  child: SparkPlan
) extends UnaryExecNode

// Join ç®—å­
case class ShuffledHashJoinExec(
  leftKeys: Seq[Expression],
  rightKeys: Seq[Expression],
  joinType: JoinType,
  buildSide: BuildSide,
  condition: Option[Expression],
  left: SparkPlan,
  right: SparkPlan
) extends BinaryExecNode
```

### 5.1.3 Physical Plan ç¤ºä¾‹

å¯¹äºæŸ¥è¯¢ï¼š
```sql
SELECT category, AVG(value) as avg_val
FROM sales
WHERE value > 100
GROUP BY category
```

ç”Ÿæˆçš„ Physical Planï¼š
```
HashAggregateExec (Final)
  hashKeys: [category#0]
  functions: [avg(value#1)]
  â†“
Exchange (Shuffle)
  hashpartitioning(category#0, 200)
  â†“
HashAggregateExec (Partial)
  hashKeys: [category#0]
  functions: [partial_avg(value#1)]
  â†“
ProjectExec
  [category#0, value#1]
  â†“
FilterExec
  (value#1 > 100)
  â†“
FileScanExec
  [category#0, value#1]
  PushedFilters: [IsNotNull(value)]
```

### 5.1.4 Expression æ ‘

Physical Plan çš„æ¯ä¸ªèŠ‚ç‚¹éƒ½åŒ…å« Expression æ ‘ï¼š

```scala
// Expression å±‚æ¬¡ç»“æ„
sealed abstract class Expression

// å¶å­èŠ‚ç‚¹ - å­—æ®µå¼•ç”¨
case class AttributeReference(
  name: String,
  dataType: DataType,
  ...
) extends LeafExpression

// å¶å­èŠ‚ç‚¹ - å­—é¢é‡
case class Literal(
  value: Any,
  dataType: DataType
) extends LeafExpression

// ä¸€å…ƒè¡¨è¾¾å¼
case class UnaryMinus(child: Expression) extends UnaryExpression

// äºŒå…ƒè¡¨è¾¾å¼
case class Add(left: Expression, right: Expression) extends BinaryExpression

// å‡½æ•°è°ƒç”¨
case class ScalarFunction(
  name: String,
  arguments: Seq[Expression]
) extends Expression
```

**ç¤ºä¾‹**ï¼š`value > 100` çš„ Expression æ ‘ï¼š
```
GreaterThan
  â”œâ”€ AttributeReference("value", IntegerType)
  â””â”€ Literal(100, IntegerType)
```

## 5.2 Substrait è§„èŒƒè¯¦è§£

### 5.2.1 Substrait ç®€ä»‹

**Substrait** æ˜¯ä¸€ä¸ªè·¨è¯­è¨€ã€è·¨å¼•æ“çš„æŸ¥è¯¢è®¡åˆ’è¡¨ç¤ºæ ‡å‡†ã€‚

**è®¾è®¡ç›®æ ‡**ï¼š
- ğŸ“‹ **æ ‡å‡†åŒ–**ï¼šç»Ÿä¸€çš„è®¡åˆ’è¡¨ç¤º
- ğŸ”Œ **å¯æ‰©å±•**ï¼šæ”¯æŒè‡ªå®šä¹‰æ‰©å±•
- ğŸš€ **é«˜æ•ˆ**ï¼šProtocol Buffers åºåˆ—åŒ–
- ğŸŒ **è·¨å¹³å°**ï¼šè¯­è¨€æ— å…³

### 5.2.2 Substrait æ ¸å¿ƒæ¦‚å¿µ

#### 1. Plan (è®¡åˆ’)

```protobuf
message Plan {
  // ç‰ˆæœ¬ä¿¡æ¯
  Version version = 1;
  
  // æ‰©å±•ä¿¡æ¯
  repeated Extension extensions = 2;
  
  // å…³ç³»ï¼ˆæŸ¥è¯¢æ“ä½œï¼‰
  repeated PlanRel relations = 3;
  
  // é«˜çº§æ‰©å±•
  google.protobuf.Any advanced_extensions = 4;
}
```

#### 2. Relation (å…³ç³»)

Relation æ˜¯æŸ¥è¯¢è®¡åˆ’çš„åŸºæœ¬å•å…ƒï¼š

```protobuf
message Rel {
  oneof rel_type {
    ReadRel read = 1;           // è¯»å–æ“ä½œ
    FilterRel filter = 2;       // è¿‡æ»¤æ“ä½œ
    ProjectRel project = 3;     // æŠ•å½±æ“ä½œ
    AggregateRel aggregate = 4; // èšåˆæ“ä½œ
    JoinRel join = 5;           // è¿æ¥æ“ä½œ
    SortRel sort = 6;           // æ’åºæ“ä½œ
    // ... æ›´å¤šå…³ç³»ç±»å‹
  }
}
```

#### 3. Expression (è¡¨è¾¾å¼)

```protobuf
message Expression {
  oneof expression_type {
    Literal literal = 1;              // å­—é¢é‡
    Selection selection = 2;          // å­—æ®µé€‰æ‹©
    ScalarFunction scalar_function = 3; // æ ‡é‡å‡½æ•°
    Cast cast = 4;                    // ç±»å‹è½¬æ¢
    // ... æ›´å¤šè¡¨è¾¾å¼ç±»å‹
  }
}
```

### 5.2.3 å¸¸ç”¨ Relation è¯¦è§£

#### ReadRel (è¯»å–)

```protobuf
message ReadRel {
  // åŸºç¡€ Schema
  NamedStruct base_schema = 1;
  
  // æ•°æ®æº
  oneof read_type {
    VirtualTable virtual_table = 2;
    LocalFiles local_files = 3;
    NamedTable named_table = 4;
  }
  
  // è¿‡æ»¤æ¡ä»¶ï¼ˆè°“è¯ä¸‹æ¨ï¼‰
  Expression filter = 5;
  
  // æŠ•å½±ï¼ˆåˆ—è£å‰ªï¼‰
  MaskExpression projection = 6;
}
```

**å¯¹åº” Spark**ï¼š`FileScanExec`

#### FilterRel (è¿‡æ»¤)

```protobuf
message FilterRel {
  // è¾“å…¥å…³ç³»
  Rel input = 1;
  
  // è¿‡æ»¤æ¡ä»¶
  Expression condition = 2;
}
```

**å¯¹åº” Spark**ï¼š`FilterExec`

#### ProjectRel (æŠ•å½±)

```protobuf
message ProjectRel {
  // è¾“å…¥å…³ç³»
  Rel input = 1;
  
  // æŠ•å½±è¡¨è¾¾å¼åˆ—è¡¨
  repeated Expression expressions = 2;
}
```

**å¯¹åº” Spark**ï¼š`ProjectExec`

#### AggregateRel (èšåˆ)

```protobuf
message AggregateRel {
  // è¾“å…¥å…³ç³»
  Rel input = 1;
  
  // åˆ†ç»„é”®
  repeated Expression groupings = 2;
  
  // èšåˆå‡½æ•°
  repeated AggregateFunction measures = 3;
}

message AggregateFunction {
  // å‡½æ•°å¼•ç”¨
  uint32 function_reference = 1;
  
  // å‚æ•°
  repeated Expression arguments = 2;
  
  // èšåˆé˜¶æ®µï¼ˆINITIAL, INTERMEDIATE, FINALï¼‰
  AggregationPhase phase = 3;
}
```

**å¯¹åº” Spark**ï¼š`HashAggregateExec`

#### JoinRel (è¿æ¥)

```protobuf
message JoinRel {
  // å·¦ä¾§è¾“å…¥
  Rel left = 1;
  
  // å³ä¾§è¾“å…¥
  Rel right = 2;
  
  // è¿æ¥æ¡ä»¶
  Expression expression = 3;
  
  // è¿æ¥ç±»å‹
  JoinType type = 4;
  
  // åè¿‡æ»¤æ¡ä»¶
  Expression post_join_filter = 5;
}

enum JoinType {
  JOIN_TYPE_UNSPECIFIED = 0;
  JOIN_TYPE_INNER = 1;
  JOIN_TYPE_OUTER = 2;
  JOIN_TYPE_LEFT = 3;
  JOIN_TYPE_RIGHT = 4;
  // ...
}
```

**å¯¹åº” Spark**ï¼š`HashJoinExec`, `SortMergeJoinExec`

### 5.2.4 å‡½æ•°æ‰©å±•æœºåˆ¶

Substrait é€šè¿‡æ‰©å±•æœºåˆ¶æ”¯æŒè‡ªå®šä¹‰å‡½æ•°ï¼š

```protobuf
message Extension {
  // æ‰©å±• URI
  string extension_uri_reference = 1;
  
  // æ‰©å±•å‡½æ•°
  repeated SimpleExtensionDeclaration extension_function = 2;
}

message SimpleExtensionDeclaration {
  // å‡½æ•°æ˜ å°„
  message ExtensionFunction {
    uint32 function_anchor = 1;  // å‡½æ•° ID
    string name = 2;               // å‡½æ•°å
  }
}
```

**ç¤ºä¾‹**ï¼šæ³¨å†Œ `regexp_extract` å‡½æ•°
```json
{
  "extension_uri_reference": "/functions_string.yaml",
  "extension_function": {
    "function_anchor": 42,
    "name": "regexp_extract"
  }
}
```

## 5.3 ä» Spark Plan åˆ° Substrait Plan çš„è½¬æ¢

### 5.3.1 è½¬æ¢æ€»ä½“æµç¨‹

```mermaid
graph TD
    A[Spark Physical Plan] --> B{æ˜¯å¦æ”¯æŒè½¬æ¢?}
    B -->|æ˜¯| C[åˆ›å»º Transformer]
    B -->|å¦| D[æ ‡è®° Fallback]
    C --> E[éå†å­èŠ‚ç‚¹]
    E --> F[è½¬æ¢ Expression]
    F --> G[ç”Ÿæˆ Substrait Relation]
    G --> H[æ„å»º Substrait Plan]
    H --> I[åºåˆ—åŒ–ä¸º Protobuf]
    D --> J[ä¿æŒåŸ Spark ç®—å­]
```

### 5.3.2 Transformer åŸºç±»

æ‰€æœ‰ç®—å­çš„ Transformer éƒ½ç»§æ‰¿è‡ªåŸºç±»ï¼š

```scala
abstract class TransformSupport extends SparkPlan {
  // æ˜¯å¦æ”¯æŒè½¬æ¢
  def supportTransform(): Boolean
  
  // è½¬æ¢ä¸º Substrait
  def toSubstraitRel(context: SubstraitContext): SubstraitRel
  
  // è·å–è¾“å‡ºå±æ€§
  override def output: Seq[Attribute]
  
  // åˆ—å¼æ‰§è¡Œ
  override def executeColumnar(): RDD[ColumnarBatch]
}
```

### 5.3.3 å…·ä½“ç®—å­è½¬æ¢

#### FilterExec è½¬æ¢

```scala
case class FilterExecTransformer(
  condition: Expression,
  child: SparkPlan
) extends UnaryExecNode with TransformSupport {
  
  override def toSubstraitRel(context: SubstraitContext): SubstraitRel = {
    // 1. è½¬æ¢å­èŠ‚ç‚¹
    val input = child.asInstanceOf[TransformSupport]
      .toSubstraitRel(context)
    
    // 2. è½¬æ¢è¿‡æ»¤æ¡ä»¶
    val conditionExpr = ExpressionConverter
      .toSubstraitExpression(condition, child.output, context)
    
    // 3. æ„å»º FilterRel
    val filterRel = FilterRel.newBuilder()
      .setInput(input)
      .setCondition(conditionExpr)
      .build()
    
    // 4. åŒ…è£…ä¸º Rel
    Rel.newBuilder()
      .setFilter(filterRel)
      .build()
  }
}
```

#### ProjectExec è½¬æ¢

```scala
case class ProjectExecTransformer(
  projectList: Seq[NamedExpression],
  child: SparkPlan
) extends UnaryExecNode with TransformSupport {
  
  override def toSubstraitRel(context: SubstraitContext): SubstraitRel = {
    // 1. è½¬æ¢å­èŠ‚ç‚¹
    val input = child.asInstanceOf[TransformSupport]
      .toSubstraitRel(context)
    
    // 2. è½¬æ¢æŠ•å½±è¡¨è¾¾å¼åˆ—è¡¨
    val expressions = projectList.map { expr =>
      ExpressionConverter.toSubstraitExpression(
        expr, child.output, context
      )
    }
    
    // 3. æ„å»º ProjectRel
    val projectRel = ProjectRel.newBuilder()
      .setInput(input)
      .addAllExpressions(expressions.asJava)
      .build()
    
    Rel.newBuilder()
      .setProject(projectRel)
      .build()
  }
}
```

#### HashAggregateExec è½¬æ¢

```scala
case class HashAggregateExecTransformer(
  groupingExpressions: Seq[NamedExpression],
  aggregateExpressions: Seq[AggregateExpression],
  child: SparkPlan
) extends UnaryExecNode with TransformSupport {
  
  override def toSubstraitRel(context: SubstraitContext): SubstraitRel = {
    // 1. è½¬æ¢å­èŠ‚ç‚¹
    val input = child.asInstanceOf[TransformSupport]
      .toSubstraitRel(context)
    
    // 2. è½¬æ¢åˆ†ç»„é”®
    val groupings = groupingExpressions.map { expr =>
      ExpressionConverter.toSubstraitExpression(
        expr, child.output, context
      )
    }
    
    // 3. è½¬æ¢èšåˆå‡½æ•°
    val measures = aggregateExpressions.map { aggExpr =>
      val aggFunc = aggExpr.aggregateFunction
      
      // æ³¨å†Œå‡½æ•°
      val funcId = context.registerFunction(
        aggFunc.prettyName
      )
      
      // æ„å»º AggregateFunction
      AggregateFunction.newBuilder()
        .setFunctionReference(funcId)
        .addAllArguments(
          aggFunc.children.map(e => 
            ExpressionConverter.toSubstraitExpression(e, child.output, context)
          ).asJava
        )
        .setPhase(getAggregatePhase(aggExpr))
        .build()
    }
    
    // 4. æ„å»º AggregateRel
    val aggregateRel = AggregateRel.newBuilder()
      .setInput(input)
      .addAllGroupings(groupings.asJava)
      .addAllMeasures(measures.asJava)
      .build()
    
    Rel.newBuilder()
      .setAggregate(aggregateRel)
      .build()
  }
  
  private def getAggregatePhase(expr: AggregateExpression): AggregationPhase = {
    expr.mode match {
      case Partial => AggregationPhase.INITIAL
      case PartialMerge => AggregationPhase.INTERMEDIATE
      case Final => AggregationPhase.FINAL
    }
  }
}
```

### 5.3.4 Expression è½¬æ¢

Expression çš„è½¬æ¢æ˜¯é€’å½’è¿‡ç¨‹ï¼š

```scala
object ExpressionConverter {
  def toSubstraitExpression(
    expr: Expression,
    inputSchema: Seq[Attribute],
    context: SubstraitContext
  ): SubstraitExpression = {
    
    expr match {
      // 1. å­—é¢é‡
      case Literal(value, dataType) =>
        SubstraitExpression.newBuilder()
          .setLiteral(toLiteralValue(value, dataType))
          .build()
      
      // 2. å­—æ®µå¼•ç”¨
      case attr: AttributeReference =>
        val fieldIndex = inputSchema.indexWhere(_.exprId == attr.exprId)
        SubstraitExpression.newBuilder()
          .setSelection(
            FieldReference.newBuilder()
              .setDirectReference(
                Reference.newBuilder()
                  .setStructField(
                    StructField.newBuilder()
                      .setField(fieldIndex)
                  )
              )
          )
          .build()
      
      // 3. äºŒå…ƒè¡¨è¾¾å¼
      case Add(left, right) =>
        val funcId = context.registerFunction("add")
        SubstraitExpression.newBuilder()
          .setScalarFunction(
            ScalarFunction.newBuilder()
              .setFunctionReference(funcId)
              .addArguments(toSubstraitExpression(left, inputSchema, context))
              .addArguments(toSubstraitExpression(right, inputSchema, context))
          )
          .build()
      
      // 4. æ¯”è¾ƒè¡¨è¾¾å¼
      case GreaterThan(left, right) =>
        val funcId = context.registerFunction("gt")
        SubstraitExpression.newBuilder()
          .setScalarFunction(
            ScalarFunction.newBuilder()
              .setFunctionReference(funcId)
              .addArguments(toSubstraitExpression(left, inputSchema, context))
              .addArguments(toSubstraitExpression(right, inputSchema, context))
          )
          .build()
      
      // 5. å‡½æ•°è°ƒç”¨
      case ScalarFunction(name, args, _) =>
        val funcId = context.registerFunction(name)
        val substraitArgs = args.map(
          toSubstraitExpression(_, inputSchema, context)
        )
        SubstraitExpression.newBuilder()
          .setScalarFunction(
            ScalarFunction.newBuilder()
              .setFunctionReference(funcId)
              .addAllArguments(substraitArgs.asJava)
          )
          .build()
      
      // 6. CAST è¡¨è¾¾å¼
      case Cast(child, dataType, _) =>
        SubstraitExpression.newBuilder()
          .setCast(
            Cast.newBuilder()
              .setInput(toSubstraitExpression(child, inputSchema, context))
              .setType(toSubstraitType(dataType))
          )
          .build()
      
      // å…¶ä»–è¡¨è¾¾å¼...
      case _ =>
        throw new UnsupportedOperationException(
          s"Unsupported expression: ${expr.getClass.getName}"
        )
    }
  }
  
  // æ•°æ®ç±»å‹è½¬æ¢
  private def toSubstraitType(sparkType: DataType): Type = {
    sparkType match {
      case IntegerType => Type.newBuilder().setI32(Type.I32.newBuilder()).build()
      case LongType => Type.newBuilder().setI64(Type.I64.newBuilder()).build()
      case DoubleType => Type.newBuilder().setFp64(Type.FP64.newBuilder()).build()
      case StringType => Type.newBuilder().setString(Type.String.newBuilder()).build()
      // ... æ›´å¤šç±»å‹
    }
  }
}
```

### 5.3.5 å®Œæ•´è½¬æ¢ç¤ºä¾‹

å¯¹äºå‰é¢çš„ SQL æŸ¥è¯¢ï¼Œå®Œæ•´çš„è½¬æ¢è¿‡ç¨‹ï¼š

```scala
// 1. Spark Physical Plan
val sparkPlan: SparkPlan = 
  HashAggregateExec(
    groupingExpressions = Seq(col("category")),
    aggregateExpressions = Seq(avg(col("value"))),
    child = FilterExec(
      condition = col("value") > 100,
      child = FileScanExec(...)
    )
  )

// 2. è½¬æ¢ä¸º Gluten Transformer
val glutenPlan = 
  HashAggregateExecTransformer(
    groupingExpressions = Seq(col("category")),
    aggregateExpressions = Seq(avg(col("value"))),
    child = FilterExecTransformer(
      condition = col("value") > 100,
      child = FileScanTransformer(...)
    )
  )

// 3. ç”Ÿæˆ Substrait Plan
val context = new SubstraitContext()
val substraitRel = glutenPlan.toSubstraitRel(context)

// 4. æ„å»ºå®Œæ•´çš„ Plan
val substraitPlan = Plan.newBuilder()
  .setVersion(Version.newBuilder().setMinorNumber(42))
  .addExtensions(context.getExtensions())
  .addRelations(
    PlanRel.newBuilder()
      .setRoot(
        RelRoot.newBuilder()
          .setInput(substraitRel)
      )
  )
  .build()

// 5. åºåˆ—åŒ–
val planBytes = substraitPlan.toByteArray
```

ç”Ÿæˆçš„ Substrait Plan (ç®€åŒ–çš„ JSON è¡¨ç¤º)ï¼š
```json
{
  "version": {"minor_number": 42},
  "extensions": [
    {"extension_function": {"function_anchor": 1, "name": "avg"}},
    {"extension_function": {"function_anchor": 2, "name": "gt"}}
  ],
  "relations": [{
    "root": {
      "input": {
        "aggregate": {
          "input": {
            "filter": {
              "input": {
                "read": {
                  "baseSchema": {...},
                  "localFiles": {...}
                }
              },
              "condition": {
                "scalarFunction": {
                  "functionReference": 2,
                  "arguments": [
                    {"selection": {"directReference": {"structField": {"field": 1}}}},
                    {"literal": {"i32": 100}}
                  ]
                }
              }
            }
          },
          "groupings": [
            {"selection": {"directReference": {"structField": {"field": 0}}}}
          ],
          "measures": [{
            "measure": {
              "functionReference": 1,
              "arguments": [
                {"selection": {"directReference": {"structField": {"field": 1}}}}
              ],
              "phase": "AGGREGATION_PHASE_INITIAL_TO_RESULT"
            }
          }]
        }
      }
    }
  }]
}
```

## 5.4 ä» Substrait Plan åˆ° Native Plan çš„è½¬æ¢

### 5.4.1 Velox ç«¯çš„å¤„ç†

åœ¨ C++ ä¾§ï¼ˆVeloxï¼‰ï¼Œæ¥æ”¶å¹¶è§£æ Substrait Planï¼š

```cpp
#include <substrait/plan.pb.h>
#include <velox/exec/PlanNode.h>

namespace gluten {

class SubstraitToVeloxPlanConverter {
public:
  // è½¬æ¢å…¥å£
  std::shared_ptr<velox::core::PlanNode> 
  toVeloxPlan(const substrait::Plan& plan) {
    // 1. æå– Root Relation
    auto rootRel = plan.relations(0).root();
    
    // 2. è½¬æ¢ Relation
    return convertRel(rootRel.input());
  }

private:
  // é€’å½’è½¬æ¢ Relation
  std::shared_ptr<velox::core::PlanNode> 
  convertRel(const substrait::Rel& rel) {
    
    if (rel.has_read()) {
      return convertRead(rel.read());
    } else if (rel.has_filter()) {
      return convertFilter(rel.filter());
    } else if (rel.has_project()) {
      return convertProject(rel.project());
    } else if (rel.has_aggregate()) {
      return convertAggregate(rel.aggregate());
    } else if (rel.has_join()) {
      return convertJoin(rel.join());
    }
    // ... å…¶ä»–ç±»å‹
    
    throw std::runtime_error("Unsupported relation type");
  }
  
  // è½¬æ¢ Filter
  std::shared_ptr<velox::core::FilterNode> 
  convertFilter(const substrait::FilterRel& filterRel) {
    // 1. è½¬æ¢è¾“å…¥
    auto input = convertRel(filterRel.input());
    
    // 2. è½¬æ¢è¿‡æ»¤æ¡ä»¶
    auto condition = convertExpression(
      filterRel.condition(), 
      input->outputType()
    );
    
    // 3. åˆ›å»º FilterNode
    return std::make_shared<velox::core::FilterNode>(
      nextPlanNodeId(),
      condition,
      input
    );
  }
  
  // è½¬æ¢ Aggregate
  std::shared_ptr<velox::core::AggregationNode> 
  convertAggregate(const substrait::AggregateRel& aggRel) {
    // 1. è½¬æ¢è¾“å…¥
    auto input = convertRel(aggRel.input());
    
    // 2. è½¬æ¢åˆ†ç»„é”®
    std::vector<velox::core::FieldAccessTypedExprPtr> groupingKeys;
    for (const auto& grouping : aggRel.groupings()) {
      groupingKeys.push_back(
        convertFieldReference(grouping, input->outputType())
      );
    }
    
    // 3. è½¬æ¢èšåˆå‡½æ•°
    std::vector<velox::core::AggregationNode::Aggregate> aggregates;
    for (const auto& measure : aggRel.measures()) {
      auto aggFunc = measure.measure();
      
      // è·å–å‡½æ•°å
      std::string funcName = getFunctionName(
        aggFunc.function_reference()
      );
      
      // è½¬æ¢å‚æ•°
      std::vector<velox::core::TypedExprPtr> args;
      for (const auto& arg : aggFunc.arguments()) {
        args.push_back(convertExpression(arg, input->outputType()));
      }
      
      // åˆ›å»ºèšåˆ
      aggregates.push_back({
        .call = std::make_shared<velox::core::CallTypedExpr>(
          funcName,
          args,
          getResultType(funcName, args)
        ),
        .rawInputTypes = getInputTypes(args)
      });
    }
    
    // 4. åˆ›å»º AggregationNode
    return std::make_shared<velox::core::AggregationNode>(
      nextPlanNodeId(),
      velox::core::AggregationNode::Step::kSingle,
      groupingKeys,
      std::vector<velox::core::FieldAccessTypedExprPtr>(), // preGroupedKeys
      std::vector<std::string>(), // aggregateNames
      aggregates,
      false, // ignoreNullKeys
      input
    );
  }
  
  // è½¬æ¢ Expression
  velox::core::TypedExprPtr 
  convertExpression(
    const substrait::Expression& expr,
    const velox::RowTypePtr& inputType
  ) {
    if (expr.has_literal()) {
      return convertLiteral(expr.literal());
    } else if (expr.has_selection()) {
      return convertFieldReference(expr.selection(), inputType);
    } else if (expr.has_scalar_function()) {
      return convertScalarFunction(expr.scalar_function(), inputType);
    } else if (expr.has_cast()) {
      return convertCast(expr.cast(), inputType);
    }
    
    throw std::runtime_error("Unsupported expression type");
  }
  
  // è½¬æ¢æ ‡é‡å‡½æ•°
  velox::core::TypedExprPtr 
  convertScalarFunction(
    const substrait::Expression::ScalarFunction& func,
    const velox::RowTypePtr& inputType
  ) {
    // 1. è·å–å‡½æ•°å
    std::string funcName = getFunctionName(func.function_reference());
    
    // 2. è½¬æ¢å‚æ•°
    std::vector<velox::core::TypedExprPtr> args;
    for (const auto& arg : func.arguments()) {
      args.push_back(convertExpression(arg, inputType));
    }
    
    // 3. åˆ›å»º CallTypedExpr
    return std::make_shared<velox::core::CallTypedExpr>(
      funcName,
      args,
      getResultType(funcName, args)
    );
  }
  
  // è¾…åŠ©æ–¹æ³•ï¼šç”Ÿæˆ Plan Node ID
  std::string nextPlanNodeId() {
    return std::to_string(planNodeId_++);
  }
  
  // å‡½æ•°æ˜ å°„
  std::string getFunctionName(uint32_t functionReference) {
    // ä»æ‰©å±•ä¸­æŸ¥æ‰¾å‡½æ•°å
    return extensionFunctions_[functionReference];
  }
  
  int planNodeId_ = 0;
  std::map<uint32_t, std::string> extensionFunctions_;
};

} // namespace gluten
```

### 5.4.2 Velox Plan Node ç»“æ„

è½¬æ¢åçš„ Velox Planï¼š

```cpp
// Velox Plan èŠ‚ç‚¹å±‚æ¬¡
auto plan = 
  std::make_shared<AggregationNode>(
    "1",
    AggregationNode::Step::kSingle,
    groupingKeys: {field("category")},
    aggregates: {
      Aggregate{
        call: CallTypedExpr("avg", {field("value")})
      }
    },
    source: std::make_shared<FilterNode>(
      "2",
      condition: CallTypedExpr("gt", {field("value"), literal(100)}),
      source: std::make_shared<TableScanNode>(
        "3",
        outputType: ROW({"category", "value"}),
        tableHandle: ...
      )
    )
  );
```

### 5.4.3 æ‰§è¡Œ Pipeline æ„å»º

Velox å°† Plan è½¬æ¢ä¸ºæ‰§è¡Œ Pipelineï¼š

```cpp
class Task {
public:
  void start() {
    // 1. æ„å»º Driver Pipeline
    auto drivers = createDrivers(planNode_);
    
    // 2. æ‰§è¡Œ Drivers
    for (auto& driver : drivers) {
      driver->run();
    }
  }

private:
  std::vector<std::unique_ptr<Driver>> 
  createDrivers(const PlanNodePtr& node) {
    // åˆ›å»º Operator Pipeline
    std::vector<std::unique_ptr<Operator>> operators;
    
    // æ·±åº¦ä¼˜å…ˆéå†
    addOperators(node, operators);
    
    // åˆ›å»º Driver
    std::vector<std::unique_ptr<Driver>> drivers;
    drivers.push_back(
      std::make_unique<Driver>(std::move(operators))
    );
    
    return drivers;
  }
  
  void addOperators(
    const PlanNodePtr& node,
    std::vector<std::unique_ptr<Operator>>& operators
  ) {
    // é€’å½’æ·»åŠ 
    for (auto& child : node->sources()) {
      addOperators(child, operators);
    }
    
    // åˆ›å»ºå¯¹åº”çš„ Operator
    if (auto filterNode = std::dynamic_pointer_cast<FilterNode>(node)) {
      operators.push_back(
        std::make_unique<FilterOperator>(filterNode)
      );
    } else if (auto aggNode = std::dynamic_pointer_cast<AggregationNode>(node)) {
      operators.push_back(
        std::make_unique<AggregationOperator>(aggNode)
      );
    }
    // ... å…¶ä»–ç±»å‹
  }
};
```

**Pipeline ç¤ºä¾‹**ï¼š
```
TableScanOperator
    â†“
FilterOperator
    â†“
AggregationOperator (Partial)
    â†“
LocalExchangeOperator
    â†“
AggregationOperator (Final)
```

## 5.5 è®¡åˆ’è½¬æ¢çš„ä¼˜åŒ–ç­–ç•¥

### 5.5.1 è°“è¯ä¸‹æ¨

å°†è¿‡æ»¤æ¡ä»¶å°½å¯èƒ½ä¸‹æ¨åˆ°æ•°æ®æºï¼š

```scala
// ä¼˜åŒ–å‰
Project
  â†“
Filter (value > 100)
  â†“
FileScan

// ä¼˜åŒ–å
Project
  â†“
FileScan (with pushed filter: value > 100)
```

**å®ç°**ï¼š
```scala
object PushDownFilters extends Rule[SparkPlan] {
  def apply(plan: SparkPlan): SparkPlan = plan transform {
    case FilterExec(condition, scan: FileScanExec) 
      if canPushDown(condition, scan) =>
      
      // å°†è¿‡æ»¤æ¡ä»¶æ·»åŠ åˆ° FileScan
      scan.copy(
        dataFilters = scan.dataFilters :+ condition
      )
  }
}
```

### 5.5.2 åˆ—è£å‰ª

åªè¯»å–éœ€è¦çš„åˆ—ï¼š

```scala
// åŸå§‹æŸ¥è¯¢é€‰æ‹©æ‰€æœ‰åˆ—
SELECT category, value FROM table

// ä¼˜åŒ–ï¼šåªè¯»å–éœ€è¦çš„åˆ—
FileScan: [category#0, value#1]  // ä¸è¯»å–å…¶ä»–åˆ—
```

### 5.5.3 Projection æŠ˜å 

åˆå¹¶è¿ç»­çš„ Projectionï¼š

```scala
// ä¼˜åŒ–å‰
Project [category, avg_value * 2]
  â†“
Project [category, avg(value) as avg_value]
  â†“
...

// ä¼˜åŒ–å
Project [category, avg(value) * 2]
  â†“
...
```

**å®ç°**ï¼š
```scala
object CollapseProject extends Rule[SparkPlan] {
  def apply(plan: SparkPlan): SparkPlan = plan transform {
    case ProjectExec(list1, ProjectExec(list2, child)) =>
      // åˆå¹¶ä¸¤ä¸ª Project
      val newList = list1.map { expr =>
        expr.transform {
          case attr: Attribute =>
            list2.find(_.exprId == attr.exprId)
              .map(_.child)
              .getOrElse(attr)
        }
      }
      ProjectExec(newList, child)
  }
}
```

### 5.5.4 èšåˆä¼˜åŒ–

**Partial Aggregation**ï¼š

```scala
// ä¸¤é˜¶æ®µèšåˆ
HashAggregate (Final)
  â†“
Exchange (Shuffle)
  â†“
HashAggregate (Partial)  // é¢„èšåˆï¼Œå‡å°‘ Shuffle æ•°æ®é‡
  â†“
Scan
```

### 5.5.5 Join ä¼˜åŒ–

**Broadcast Join æ£€æµ‹**ï¼š

```scala
if (rightTableSize < broadcastThreshold) {
  // ä½¿ç”¨ Broadcast Join
  BroadcastHashJoinExec(...)
} else {
  // ä½¿ç”¨ Shuffled Join
  ShuffledHashJoinExec(...)
}
```

## 5.6 ä¸æ”¯æŒç®—å­çš„å¤„ç†ï¼ˆFallbackï¼‰

### 5.6.1 æ”¯æŒæ€§æ£€æŸ¥

åœ¨è½¬æ¢å‰ï¼Œå…ˆæ£€æŸ¥ç®—å­æ˜¯å¦æ”¯æŒï¼š

```scala
object SupportChecker {
  def isSupported(plan: SparkPlan): Boolean = plan match {
    // æ”¯æŒçš„ç®—å­
    case _: FilterExec => true
    case _: ProjectExec => true
    case _: HashAggregateExec => true
    case _: ShuffledHashJoinExec => true
    
    // ä¸æ”¯æŒçš„ç®—å­
    case _: SortMergeJoinExec => false
    case _: WindowExec => checkWindowSupport(plan)
    
    // é€’å½’æ£€æŸ¥å­èŠ‚ç‚¹
    case other =>
      other.children.forall(isSupported)
  }
  
  def checkWindowSupport(plan: SparkPlan): Boolean = {
    // æ£€æŸ¥ Window å‡½æ•°æ˜¯å¦éƒ½æ”¯æŒ
    plan match {
      case window: WindowExec =>
        window.windowExpression.forall { expr =>
          isSupportedWindowFunction(expr)
        }
      case _ => true
    }
  }
}
```

### 5.6.2 Fallback æ ‡è®°

ä¸æ”¯æŒçš„ç®—å­ä¿æŒåŸæ ·ï¼š

```scala
// è½¬æ¢è§„åˆ™
object GlutenTransformStrategy extends Strategy {
  def apply(plan: LogicalPlan): Seq[SparkPlan] = {
    val physicalPlan = defaultStrategy(plan)
    
    // å°è¯•è½¬æ¢
    val transformedPlan = tryTransform(physicalPlan)
    
    Seq(transformedPlan)
  }
  
  private def tryTransform(plan: SparkPlan): SparkPlan = {
    if (SupportChecker.isSupported(plan)) {
      // è½¬æ¢ä¸º Gluten ç®—å­
      plan match {
        case filter: FilterExec =>
          FilterExecTransformer(filter.condition, tryTransform(filter.child))
        case project: ProjectExec =>
          ProjectExecTransformer(project.projectList, tryTransform(project.child))
        // ... å…¶ä»–ç®—å­
      }
    } else {
      // ä¿æŒåŸ Spark ç®—å­ï¼Œä½†éœ€è¦æ·»åŠ è½¬æ¢å±‚
      insertColumnarToRow(plan)
    }
  }
}
```

### 5.6.3 æ•°æ®æ ¼å¼è½¬æ¢

Fallback æ—¶éœ€è¦åœ¨åˆ—å¼å’Œè¡Œå¼ä¹‹é—´è½¬æ¢ï¼š

```scala
// æ’å…¥ ColumnarToRow
def insertColumnarToRow(plan: SparkPlan): SparkPlan = {
  if (plan.supportsColumnar) {
    ColumnarToRowExec(plan)
  } else {
    plan.withNewChildren(
      plan.children.map { child =>
        if (child.supportsColumnar) {
          ColumnarToRowExec(child)
        } else {
          child
        }
      }
    )
  }
}
```

**Fallback ç¤ºä¾‹**ï¼š
```
HashAggregateExecTransformer (Gluten)
  â†“
ColumnarToRow  â† è½¬æ¢ç‚¹
  â†“
WindowExec (Spark, ä¸æ”¯æŒ)
  â†“
RowToColumnar  â† è½¬æ¢ç‚¹
  â†“
ProjectExecTransformer (Gluten)
```

## æœ¬ç« å°ç»“

æœ¬ç« æ·±å…¥å­¦ä¹ äº†æŸ¥è¯¢è®¡åˆ’è½¬æ¢ï¼š

1. âœ… **Spark Physical Plan**ï¼šç†è§£äº† Spark çš„ç‰©ç†è®¡åˆ’ç»“æ„å’Œ Expression æ ‘
2. âœ… **Substrait è§„èŒƒ**ï¼šæŒæ¡äº† Substrait çš„æ ¸å¿ƒæ¦‚å¿µå’Œ Protocol Buffers è¡¨ç¤º
3. âœ… **Spark â†’ Substrait**ï¼šå­¦ä¹ äº†å¦‚ä½•å°† Spark ç®—å­å’Œè¡¨è¾¾å¼è½¬æ¢ä¸º Substrait
4. âœ… **Substrait â†’ Native**ï¼šäº†è§£äº† Velox å¦‚ä½•è§£æ Substrait å¹¶æ„å»ºæ‰§è¡Œ Pipeline
5. âœ… **ä¼˜åŒ–ç­–ç•¥**ï¼šæŒæ¡äº†è°“è¯ä¸‹æ¨ã€åˆ—è£å‰ªã€æŠ•å½±æŠ˜å ç­‰ä¼˜åŒ–æŠ€æœ¯
6. âœ… **Fallback å¤„ç†**ï¼šç†è§£äº†ä¸æ”¯æŒç®—å­çš„è¯†åˆ«å’Œå¤„ç†æœºåˆ¶

ä¸‹ä¸€ç« æˆ‘ä»¬å°†æ·±å…¥å†…å­˜ç®¡ç†ï¼Œå­¦ä¹  Gluten å¦‚ä½•ç®¡ç† Off-Heap å†…å­˜ã€‚

## å‚è€ƒèµ„æ–™

- [Spark Catalyst Optimizer](https://databricks.com/glossary/catalyst-optimizer)
- [Substrait Specification](https://substrait.io/)
- [Velox Plan Nodes](https://facebookincubator.github.io/velox/develop/programming-guide.html)
- [Protocol Buffers](https://protobuf.dev/)

---

**ä¸‹ä¸€ç« é¢„å‘Š**ï¼š[ç¬¬6ç« ï¼šå†…å­˜ç®¡ç†](chapter06-memory-management.md) - æ·±å…¥ Gluten çš„å†…å­˜ç®¡ç†æœºåˆ¶
